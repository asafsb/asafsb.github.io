<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VMWonAWS | Software-Defined Coffee</title>
    <link>https://softwaredefinedcoffee.com/categories/vmwonaws/</link>
      <atom:link href="https://softwaredefinedcoffee.com/categories/vmwonaws/index.xml" rel="self" type="application/rss+xml" />
    <description>VMWonAWS</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Software-Defined Coffee</copyright><lastBuildDate>Thu, 16 Jul 2020 20:44:10 -0500</lastBuildDate>
    <image>
      <url>https://softwaredefinedcoffee.com/img/SDCoffeeLogo.png</url>
      <title>VMWonAWS</title>
      <link>https://softwaredefinedcoffee.com/categories/vmwonaws/</link>
    </image>
    
    <item>
      <title>PowerShell Module to Manage Workload Placement in VMware Cloud on AWS Stretched Clusters</title>
      <link>https://softwaredefinedcoffee.com/post/2020-07-16-powershell-module-to-manage-workload-placement-in-vmwonaws-stretched-clusters/</link>
      <pubDate>Thu, 16 Jul 2020 20:44:10 -0500</pubDate>
      <guid>https://softwaredefinedcoffee.com/post/2020-07-16-powershell-module-to-manage-workload-placement-in-vmwonaws-stretched-clusters/</guid>
      <description>&lt;p&gt;Stretched Clusters provide the ability to protect an SDDC running in VMware Cloud on AWS from an Availability Zone failure. If you would like to know more about the Stretched Clusters capability, sometimes referred to as Multi AZ SDDCs in VMW on AWS, make sure you read &lt;ins&gt;
&lt;a href=&#34;https://cloud.vmware.com/community/2018/05/15/stretched-clusters-vmware-cloud-aws-overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://emadyounis.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emad Younis&lt;/a&gt;&lt;/ins&gt;, &lt;ins&gt;
&lt;a href=&#34;https://frankdenneman.nl/2018/05/16/stretched-clusters-vmware-cloud-aws-really-big-thing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://frankdenneman.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank Denneman&lt;/a&gt;&lt;/ins&gt; and &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/virtualblocks/2018/05/15/relentless-availability-powered-vsan-stretched-clusters-vmware-cloud-aws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://twitter.com/glnsize&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glenn Sizemore&lt;/a&gt;&lt;/ins&gt;. In addition, &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/virtualblocks/2019/12/03/reduced-pricing-stretched-clusters-vmware-cloud-aws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;as announced in December 2019&lt;/a&gt;&lt;/ins&gt;, there is a 95% discount on the cross-AZ traffic between the AZs.&lt;/p&gt;
&lt;p&gt;Just like a standard vSAN cluster, storage consumption in a Stretched Cluster is managed using storage policies. The &lt;strong&gt;Failures to tolerate&lt;/strong&gt; settings defines how data is placed in a single AZ, e.g. RAID1 FTT1, while &lt;strong&gt;Site disaster tolerance&lt;/strong&gt; configures how data is placed across the AZs. The workload can either be mirrored across sites or set to a specific site (preferred or non-preferred) with no cross-site protection. Logically, most workloads in a Stretched Cluster will be protected across both sites. However, there may be some cases where vSAN cross-AZ protection is unnecessary, such as workloads that already offer application level redundancy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;Stretched Cluster Storage Policy&#34;&gt;&lt;/p&gt;
&lt;p&gt;One aspect that is sometimes overlooked is that the storage policies only handle data placement while the placement of compute is handled by DRS. By default, DRS has no awareness of where the storage of a specific workload resides and whether it&amp;rsquo;s replicated across the sites. This can result in a misalignment of the data and compute placement. For example, the storage policy can configure data to be placed in the preferred site while DRS places or moves the VM to a host in the non-preferred one. Another potential scenario is a clustered application that can have both instances running on hosts in the same AZ which increases recovery time in case of an AZ failure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image2.png&#34; alt=&#34;Stretched Cluster Data and Compute Misalignment&#34;&gt;&lt;/p&gt;
&lt;p&gt;On-prem, DRS affinity rules are leveraged to make DRS aware of vSAN storage placement. In VMW on AWS the same outcome can be achieved using tags and &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/vsphere/2019/08/vmware-cloud-on-aws-policies.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compute Policies&lt;/a&gt;&lt;/ins&gt;. Basically, two VM-Host compute policies should be created – one for the preferred site and on for the non-preferred site. After that all the hosts in the corresponding fault-domains will need to be tagged. In addition, every time a host is added to the cluster it will need to be tagged otherwise no VMs from these compute policies will be running on the new host. It is possible to create the compute policies manually and then tag all the hosts, but to make the process easier and faster I created a PowerShell module named &lt;ins&gt;
&lt;a href=&#34;https://github.com/asafsb/sdcoffee-scripts/tree/master/MultiAZ-Tagging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MultiAZTagging.psm1&lt;/a&gt;&lt;/ins&gt; to create the policies and tag the hosts.&lt;/p&gt;
&lt;h3 id=&#34;powershell-module-review&#34;&gt;PowerShell Module Review&lt;/h3&gt;
&lt;p&gt;The module contains two cmdlets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;New-MultiAzTags&lt;/strong&gt; - Creates a tag category, tags and compute policies for the preferred and non-preferred sites. By default, all the objects created are prepended with “MultiAZ”. A different string can be specified using the &lt;em&gt;&lt;strong&gt;TagCategoryName&lt;/strong&gt;&lt;/em&gt; parameter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; – Assigns the appropriate tags to hosts in the cluster. A specific host or cluster can be specified, using either &lt;em&gt;&lt;strong&gt;HostName&lt;/strong&gt;&lt;/em&gt; or &lt;em&gt;&lt;strong&gt;ClusterName&lt;/strong&gt;&lt;/em&gt;, otherwise all the hosts in the SDDC will be tagged. If the right tag exists on the host no action is taken. In case a different value other than “MultiAZ” was used to create the tags it should be specified using the &lt;em&gt;&lt;strong&gt;TagCategoryName&lt;/strong&gt;&lt;/em&gt; parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-the-multiaz-tagging-module&#34;&gt;Using the MultiAZ Tagging Module&lt;/h3&gt;
&lt;p&gt;Before I step into how to run the script, I wanted to give you a quick recap of the environment I&amp;rsquo;m going to run it on. This is stretched cluster SDDC deployed across two AZs in Oregon (us-west-2). The AZs are us-west-2a and us-west-2b (preferred fault domain):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image3.png&#34; alt=&#34;Test Environment Fault Domains&#34;&gt;&lt;/p&gt;
&lt;p&gt;There’s two VMs in the cluster, &lt;strong&gt;az1&lt;/strong&gt; and &lt;strong&gt;az2&lt;/strong&gt;. At the moment they both run on a host in us-west-2b, which is the preferred site:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image4.png&#34; alt=&#34;Test Environment VM Placement Pre Script&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;ins&gt;
&lt;a href=&#34;https://github.com/asafsb/sdcoffee-scripts/tree/master/MultiAZ-Tagging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download the PowerShell module&lt;/a&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Import the PowerShell module&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Import-Module .\MultiAZTagging.psm1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Connect to a vCenter in a Stretched Cluster SDDC using &lt;em&gt;&lt;strong&gt;Connect-VIServer&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image5.png&#34; alt=&#34;Connect-VIServer&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Connect to the same vCenter using &lt;em&gt;&lt;strong&gt;Connect-CisServer&lt;/strong&gt;&lt;/em&gt; (this step is required to create the compute policies)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image6.png&#34; alt=&#34;Connect-CisServer&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Run the &lt;em&gt;&lt;strong&gt;New-MultiAZTags&lt;/strong&gt;&lt;/em&gt; command to create the tag category, tags and compute policies&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image7.png&#34; alt=&#34;New-MultiAZTags Command and Output&#34;&gt;&lt;/p&gt;
&lt;p&gt;As shown by the command&amp;rsquo;s output, the following were created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tag category with the name &lt;strong&gt;MultiAZ&lt;/strong&gt;. Only one tag from this category can be assigned to a single host or VM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image8.png&#34; alt=&#34;Newly Created Tag Category&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two tags in the &lt;strong&gt;MultiAZ&lt;/strong&gt; tag category - &lt;strong&gt;MultiAZ-Preferred&lt;/strong&gt; and &lt;strong&gt;MultiAZ-Non-Preferred&lt;/strong&gt;. The description of each tag specifies the AZ for that site&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image9.png&#34; alt=&#34;Newly Created Tags&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two compute policies named &lt;strong&gt;MultiAZ-Preferred&lt;/strong&gt; and &lt;strong&gt;MultiAZ-Non-Preferred&lt;/strong&gt;. Each policy is a VM-Host affinity policy which requires the hosts and VMs to be tagged using the matching tags created previously. Note that there are zero hosts/VMs matching this policy as no host/VM has any of the tags assigned&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image10.png&#34; alt=&#34;Newly Created Compute Policies&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt; – Run the &lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; command to tag all the hosts in the SDDC&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image11.png&#34; alt=&#34;Set-MultiAzHostTag Command and Output&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As the command&amp;rsquo;s output shows, each host was assigned a tag that matches their fault domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image12.png&#34; alt=&#34;Tagged vSphere Hosts&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition, the compute policies now show that each policy has a single host with a matching tag&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image13.png&#34; alt=&#34;Compute Policies With Matching Hosts&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step-7&lt;/strong&gt; – Assign tags to the appropriate VMs. This can be done manually via the GUI or by using the &lt;strong&gt;New-TagAssignment&lt;/strong&gt; PowerCLI command.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In my environment I tagged VM az1 with the MultiAZ-Preferred tag and VM az2 with the MultiAZ-Non-Preferred tag. Following that both compute policies now show one associated VM in addition to the host&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image14.png&#34; alt=&#34;Compute Policies With Matching Hosts and VMs&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A couple of seconds after I assigned the MultiAZ-Non-Preferred tag to the az2 VM DRS initiated a migration to the host in the non-preferred fault domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image15.png&#34; alt=&#34;Test Environment VM Placement Post Script&#34;&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned before, there are several use cases I can think of for pinning a VM to a specific site. The most obvious one is a VM that is not replicated across AZs, so it makes sense to have the VM running in the same AZ. The second one is a resilient application, such as AD Domain Controllers, where making sure at least one node is always running. The great thing about compute policies is that they leverage a ‘should’ rule. That means that if an entire AZ fails, for example the non-preferred one, if the VM data is replicated the VMs set to the MultiAZ-non-preferred compute policy will still be powered on the preferred site by vSphere HA. Once the non-preferred AZ comes back up DRS will migrate the VMs to hosts the adhere to the compute policy.&lt;/p&gt;
&lt;p&gt;I hope that you find this PowerShell module useful and that it can help address some design requirements in your SDDC. At the moment, in order to tag new hosts that are added to the SDDC using eDRS or as a planned procedure, you’ll need to run &lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; again. Since it skips existing hosts you can also run it on a schedule. In the next couple of weeks, I’m planning on testing integration of the Set-MultiAzHostTag with &lt;ins&gt;
&lt;a href=&#34;https://vmweventbroker.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VEBA&lt;/a&gt;&lt;/ins&gt;, so it can be run when a new host is added to the SDDC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Configuring L2VPN High Availability in VMW on AWS</title>
      <link>https://softwaredefinedcoffee.com/post/2020-05-14-configuring-l2vpn-high-availability-in-vmw-on-aws/</link>
      <pubDate>Thu, 14 May 2020 22:17:16 -0500</pubDate>
      <guid>https://softwaredefinedcoffee.com/post/2020-05-14-configuring-l2vpn-high-availability-in-vmw-on-aws/</guid>
      <description>&lt;p&gt;When we assist customers in designing a new VMware Cloud on AWS SDDC the question of Layer 2 extension comes up frequently. The reasons to extend on-pre2301 networks are unique to each environment and can be a temporary state during migration or a long term strategy to ease scalability and bursting to the cloud.&lt;/p&gt;
&lt;p&gt;VMW on AWS provides two options for extending an on-prem network to the SDDC - HCX Network Extension (NE) and Layer 2 VPN. While both solutions provide the same functionality they are different in several aspects. I won’t go into a detailed comparison in this post, but most users, especially if they are not very familiar with networking, will find that HCX NE is easier to configure and scale. L2 VPN, on the other hand, will provide faster recoverability from an appliance failure, and depending on how many networks are extended, it may use less compute resources on-prem and in the SDDC.&lt;/p&gt;
&lt;p&gt;Since version 1.9 of VMW on AWS, the NSX-T 2.5 Autonomous Edge is used, which is simpler to configure and more performant than the previous L2 VPN client. In this post I will demonstrate how to configure the HA functionality for the autonomous edge.&lt;/p&gt;
&lt;p&gt;When configuring a highly avaialable autonomous edge two appliances are deployed. The first edge that is deployed is the Primary edge and the second one is the Secondary edge. The Primary/Secondary value is permanent and simply indicates which edge was deployed first. It doesn&amp;rsquo;t represent which edge is active and handling the L2 VPN. This is shown by Active/Standby high availability status in the summary dashboard.&lt;/p&gt;
&lt;h3 id=&#34;deploying-the-primary-autonomous-edge-and-configuring-l2-vpn&#34;&gt;Deploying the Primary Autonomous Edge and Configuring L2 VPN&lt;/h3&gt;
&lt;p&gt;My colleague, &lt;a href=&#34;https://davidwzhang.com/&#34; style=&#34;text-decoration: underline&#34;&gt; David Zhang&lt;/a&gt;, wrote &lt;a href=&#34;https://davidwzhang.com/2020/02/24/setting-up-l2vpn-in-vmc-on-aws/&#34; style=&#34;text-decoration: underline&#34;&gt; a great post&lt;/a&gt; explaining all the necessary steps to deploy L2 VPN on VMW on AWS using the autonomous edge. I followed David&amp;rsquo;s guide to configure L2 VPN and used the following values to set up the primary autonomous edge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Management IP - &lt;strong&gt;192.168.1.240&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;External Port
&lt;ul&gt;
&lt;li&gt;Port – &lt;strong&gt;0,eth1,192.168.1.241,24&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;GW – &lt;strong&gt;192.168.1.254&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Internal - &lt;strong&gt;left blank&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;HA
&lt;ul&gt;
&lt;li&gt;Port – &lt;strong&gt;0,eth3,172.25.53.1,24&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;GW – &lt;strong&gt;172.25.53.254&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Do not fill any other values in the HA section&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;Primay Autonomous Edge HA Configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;The L2 VPN tunnel and network extensions can be created before or after the secondary edge is deployed. There is no significance to the order in which these actions are performed.&lt;/p&gt;
&lt;h3 id=&#34;deploying-the-secondary-autonomous-edge&#34;&gt;Deploying the Secondary Autonomous Edge&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Deploy the autonomous edge OVF. Make sure the exact same edge size (medium or large) and networks as the primary controller are selected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Under network properties, enter the hostname, default gateway, management IP and netmask. Make sure the same subnet as the management network of the primary edge is entered. I used &lt;strong&gt;192.168.1.242&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image2.png&#34; alt=&#34;Secondary Autonomous Edge Management Network Configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - The external IP of the secondary edge can be blank. After powering on the appliance it will negotiate with the primary edge and the same external IP as the primary edge will be configured in a disconnected state. As with the primary edge the internal port should be blank:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image3.png&#34; alt=&#34;Secondary Autonomous Edge External/Internal Network Configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Configure the HA port&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Port – &lt;strong&gt;0,eth3,172.25.53.2,24&lt;/strong&gt; This port should be in the same subnet as the HA port of the primary edge&lt;/li&gt;
&lt;li&gt;Default GW – &lt;strong&gt;172.25.53.254&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Check the Secondary API Node checkbox&lt;/li&gt;
&lt;li&gt;Primary node IP – &lt;strong&gt;192.168.1.240&lt;/strong&gt; This is the management IP of the primary edge deployed previously&lt;/li&gt;
&lt;li&gt;Primary Node Credentials – If you changed the default username of the primay edge make sure you enter that username. Otherwise, it&amp;rsquo;s &lt;strong&gt;admin&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Primary node thumbprint – To get the API thumbprint login to the primary edge with the &lt;strong&gt;admin&lt;/strong&gt; user and type the following command:&lt;br&gt;&lt;br&gt;
&lt;code&gt; get certificate api thumbprint&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image4.png&#34; alt=&#34;Primay Edge API Thumbprint&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image5.png&#34; alt=&#34;Secondary Autonomous Edge HA Configuration&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Make sure the edge VMs are running on different hosts and migrate one if necessary. If DRS is configured on the cluster create a rule to separate the edges.&lt;/p&gt;
&lt;h3 id=&#34;validating-the-secondary-edge-deployment&#34;&gt;Validating The Secondary Edge Deployment&lt;/h3&gt;
&lt;p&gt;A couple of minutes after the secondary edge powered on login to both appliances to validate the High Availability status. Under HA Peer Address, each edge should list the opposite edge’s management IP as a peer.&lt;br&gt;
&lt;strong&gt;Note&lt;/strong&gt; – for some reason the management and peer addresses list a subnet mask of /19 on the web interface even though the correct mask of /24 is configured.&lt;/p&gt;
&lt;p&gt;The primary edge should have a Primary API server role and an Active HA status. Once the L2 VPN is configured its status will be Up:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image6.png&#34; alt=&#34;Primary Autonomous Edge HA Status&#34;&gt;&lt;/p&gt;
&lt;p&gt;The secondary edge should have a Secondary API server role and Standby under HA status. Once the L2 VPN is configured its status will be Doen:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image7.png&#34; alt=&#34;Secondary Autonomous Edge HA Status&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;Port&lt;/em&gt; tab you can see that both edges have port named &lt;strong&gt;lrport_0&lt;/strong&gt; with the same IP and Port ID. The GUI doesn&amp;rsquo;t indicate what is the status of the port but this can be found using CLI. To do so login to both edges with the admin user and type the command &lt;code&gt;get logical-router interfaces&lt;/code&gt; and find the port named &lt;strong&gt;lrport_0&lt;/strong&gt;. You can also type &lt;code&gt;get logical-router interface &amp;lt;port id&amp;gt;&lt;/code&gt;. The &lt;strong&gt;admin&lt;/strong&gt; state of both ports should be &lt;strong&gt;up&lt;/strong&gt; but the &lt;strong&gt;op_state&lt;/strong&gt; of the standby edge should be &lt;strong&gt;down&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Command output of the Active edge:
&lt;img src=&#34;image8.png&#34; alt=&#34;Active Autonomous Edge HA lrport Status&#34;&gt;&lt;/p&gt;
&lt;p&gt;Command output of the Stnadby edge:
&lt;img src=&#34;image9.png&#34; alt=&#34;Standby Autonomous Edge HA lrport Status&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;autonomous-edge-ha-failover-test&#34;&gt;Autonomous Edge HA Failover Test&lt;/h3&gt;
&lt;p&gt;To test the autonomous edge failover, I initiated a continuous ping with timestamp to a VM connected to an extended L2 network in VMW on AWS. After about 30 seconds to verify there are no drops, I powered off the active edge. The connection did drop but as can be seen in the screenshot below the standby edge claimed the external IP and re-established the L2 VPN tunnel in 10 seconds:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image10.png&#34; alt=&#34;HA Failover Test Ping Downtime&#34;&gt;&lt;/p&gt;
&lt;p&gt;After powering on the powered-off edge the HA status between the edges was re-established. The secondary edge remains the active edge and the primary will become active again only in case of an additional failure.&lt;/p&gt;
&lt;p&gt;Secondary edge status:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image11.png&#34; alt=&#34;Primary Autonomous Edge HA Status&#34;&gt;&lt;/p&gt;
&lt;p&gt;Primary edge status:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image12.png&#34; alt=&#34;Secondary Autonomous Edge HA Status&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
