<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vSAN | Software-Defined Coffee</title>
    <link>https://softwaredefinedcoffee.com/categories/vsan/</link>
      <atom:link href="https://softwaredefinedcoffee.com/categories/vsan/index.xml" rel="self" type="application/rss+xml" />
    <description>vSAN</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Software-Defined Coffee</copyright><lastBuildDate>Thu, 16 Jul 2020 20:44:10 -0500</lastBuildDate>
    <image>
      <url>https://softwaredefinedcoffee.com/img/SDCoffeeLogo.png</url>
      <title>vSAN</title>
      <link>https://softwaredefinedcoffee.com/categories/vsan/</link>
    </image>
    
    <item>
      <title>PowerShell Module to Manage Workload Placement in VMware Cloud on AWS Stretched Clusters</title>
      <link>https://softwaredefinedcoffee.com/post/2020-07-16-powershell-module-to-manage-workload-placement-in-vmwonaws-stretched-clusters/</link>
      <pubDate>Thu, 16 Jul 2020 20:44:10 -0500</pubDate>
      <guid>https://softwaredefinedcoffee.com/post/2020-07-16-powershell-module-to-manage-workload-placement-in-vmwonaws-stretched-clusters/</guid>
      <description>&lt;p&gt;Stretched Clusters provide the ability to protect an SDDC running in VMware Cloud on AWS from an Availability Zone failure. If you would like to know more about the Stretched Clusters capability, sometimes referred to as Multi AZ SDDCs in VMW on AWS, make sure you read &lt;ins&gt;
&lt;a href=&#34;https://cloud.vmware.com/community/2018/05/15/stretched-clusters-vmware-cloud-aws-overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://emadyounis.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emad Younis&lt;/a&gt;&lt;/ins&gt;, &lt;ins&gt;
&lt;a href=&#34;https://frankdenneman.nl/2018/05/16/stretched-clusters-vmware-cloud-aws-really-big-thing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://frankdenneman.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frank Denneman&lt;/a&gt;&lt;/ins&gt; and &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/virtualblocks/2018/05/15/relentless-availability-powered-vsan-stretched-clusters-vmware-cloud-aws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt;&lt;/ins&gt; by &lt;ins&gt;
&lt;a href=&#34;https://twitter.com/glnsize&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glenn Sizemore&lt;/a&gt;&lt;/ins&gt;. In addition, &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/virtualblocks/2019/12/03/reduced-pricing-stretched-clusters-vmware-cloud-aws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;as announced in December 2019&lt;/a&gt;&lt;/ins&gt;, there is a 95% discount on the cross-AZ traffic between the AZs.&lt;/p&gt;
&lt;p&gt;Just like a standard vSAN cluster, storage consumption in a Stretched Cluster is managed using storage policies. The &lt;strong&gt;Failures to tolerate&lt;/strong&gt; settings defines how data is placed in a single AZ, e.g. RAID1 FTT1, while &lt;strong&gt;Site disaster tolerance&lt;/strong&gt; configures how data is placed across the AZs. The workload can either be mirrored across sites or set to a specific site (preferred or non-preferred) with no cross-site protection. Logically, most workloads in a Stretched Cluster will be protected across both sites. However, there may be some cases where vSAN cross-AZ protection is unnecessary, such as workloads that already offer application level redundancy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;Stretched Cluster Storage Policy&#34;&gt;&lt;/p&gt;
&lt;p&gt;One aspect that is sometimes overlooked is that the storage policies only handle data placement while the placement of compute is handled by DRS. By default, DRS has no awareness of where the storage of a specific workload resides and whether it&amp;rsquo;s replicated across the sites. This can result in a misalignment of the data and compute placement. For example, the storage policy can configure data to be placed in the preferred site while DRS places or moves the VM to a host in the non-preferred one. Another potential scenario is a clustered application that can have both instances running on hosts in the same AZ which increases recovery time in case of an AZ failure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image2.png&#34; alt=&#34;Stretched Cluster Data and Compute Misalignment&#34;&gt;&lt;/p&gt;
&lt;p&gt;On-prem, DRS affinity rules are leveraged to make DRS aware of vSAN storage placement. In VMW on AWS the same outcome can be achieved using tags and &lt;ins&gt;
&lt;a href=&#34;https://blogs.vmware.com/vsphere/2019/08/vmware-cloud-on-aws-policies.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compute Policies&lt;/a&gt;&lt;/ins&gt;. Basically, two VM-Host compute policies should be created – one for the preferred site and on for the non-preferred site. After that all the hosts in the corresponding fault-domains will need to be tagged. In addition, every time a host is added to the cluster it will need to be tagged otherwise no VMs from these compute policies will be running on the new host. It is possible to create the compute policies manually and then tag all the hosts, but to make the process easier and faster I created a PowerShell module named &lt;ins&gt;
&lt;a href=&#34;https://github.com/asafsb/sdcoffee-scripts/tree/master/MultiAZ-Tagging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MultiAZTagging.psm1&lt;/a&gt;&lt;/ins&gt; to create the policies and tag the hosts.&lt;/p&gt;
&lt;h3 id=&#34;powershell-module-review&#34;&gt;PowerShell Module Review&lt;/h3&gt;
&lt;p&gt;The module contains two cmdlets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;New-MultiAzTags&lt;/strong&gt; - Creates a tag category, tags and compute policies for the preferred and non-preferred sites. By default, all the objects created are prepended with “MultiAZ”. A different string can be specified using the &lt;em&gt;&lt;strong&gt;TagCategoryName&lt;/strong&gt;&lt;/em&gt; parameter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; – Assigns the appropriate tags to hosts in the cluster. A specific host or cluster can be specified, using either &lt;em&gt;&lt;strong&gt;HostName&lt;/strong&gt;&lt;/em&gt; or &lt;em&gt;&lt;strong&gt;ClusterName&lt;/strong&gt;&lt;/em&gt;, otherwise all the hosts in the SDDC will be tagged. If the right tag exists on the host no action is taken. In case a different value other than “MultiAZ” was used to create the tags it should be specified using the &lt;em&gt;&lt;strong&gt;TagCategoryName&lt;/strong&gt;&lt;/em&gt; parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;using-the-multiaz-tagging-module&#34;&gt;Using the MultiAZ Tagging Module&lt;/h3&gt;
&lt;p&gt;Before I step into how to run the script, I wanted to give you a quick recap of the environment I&amp;rsquo;m going to run it on. This is stretched cluster SDDC deployed across two AZs in Oregon (us-west-2). The AZs are us-west-2a and us-west-2b (preferred fault domain):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image3.png&#34; alt=&#34;Test Environment Fault Domains&#34;&gt;&lt;/p&gt;
&lt;p&gt;There’s two VMs in the cluster, &lt;strong&gt;az1&lt;/strong&gt; and &lt;strong&gt;az2&lt;/strong&gt;. At the moment they both run on a host in us-west-2b, which is the preferred site:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image4.png&#34; alt=&#34;Test Environment VM Placement Pre Script&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;ins&gt;
&lt;a href=&#34;https://github.com/asafsb/sdcoffee-scripts/tree/master/MultiAZ-Tagging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download the PowerShell module&lt;/a&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Import the PowerShell module&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Import-Module .\MultiAZTagging.psm1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Connect to a vCenter in a Stretched Cluster SDDC using &lt;em&gt;&lt;strong&gt;Connect-VIServer&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image5.png&#34; alt=&#34;Connect-VIServer&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Connect to the same vCenter using &lt;em&gt;&lt;strong&gt;Connect-CisServer&lt;/strong&gt;&lt;/em&gt; (this step is required to create the compute policies)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image6.png&#34; alt=&#34;Connect-CisServer&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Run the &lt;em&gt;&lt;strong&gt;New-MultiAZTags&lt;/strong&gt;&lt;/em&gt; command to create the tag category, tags and compute policies&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image7.png&#34; alt=&#34;New-MultiAZTags Command and Output&#34;&gt;&lt;/p&gt;
&lt;p&gt;As shown by the command&amp;rsquo;s output, the following were created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tag category with the name &lt;strong&gt;MultiAZ&lt;/strong&gt;. Only one tag from this category can be assigned to a single host or VM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image8.png&#34; alt=&#34;Newly Created Tag Category&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two tags in the &lt;strong&gt;MultiAZ&lt;/strong&gt; tag category - &lt;strong&gt;MultiAZ-Preferred&lt;/strong&gt; and &lt;strong&gt;MultiAZ-Non-Preferred&lt;/strong&gt;. The description of each tag specifies the AZ for that site&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image9.png&#34; alt=&#34;Newly Created Tags&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two compute policies named &lt;strong&gt;MultiAZ-Preferred&lt;/strong&gt; and &lt;strong&gt;MultiAZ-Non-Preferred&lt;/strong&gt;. Each policy is a VM-Host affinity policy which requires the hosts and VMs to be tagged using the matching tags created previously. Note that there are zero hosts/VMs matching this policy as no host/VM has any of the tags assigned&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image10.png&#34; alt=&#34;Newly Created Compute Policies&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt; – Run the &lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; command to tag all the hosts in the SDDC&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image11.png&#34; alt=&#34;Set-MultiAzHostTag Command and Output&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As the command&amp;rsquo;s output shows, each host was assigned a tag that matches their fault domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image12.png&#34; alt=&#34;Tagged vSphere Hosts&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition, the compute policies now show that each policy has a single host with a matching tag&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image13.png&#34; alt=&#34;Compute Policies With Matching Hosts&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step-7&lt;/strong&gt; – Assign tags to the appropriate VMs. This can be done manually via the GUI or by using the &lt;strong&gt;New-TagAssignment&lt;/strong&gt; PowerCLI command.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In my environment I tagged VM az1 with the MultiAZ-Preferred tag and VM az2 with the MultiAZ-Non-Preferred tag. Following that both compute policies now show one associated VM in addition to the host&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image14.png&#34; alt=&#34;Compute Policies With Matching Hosts and VMs&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A couple of seconds after I assigned the MultiAZ-Non-Preferred tag to the az2 VM DRS initiated a migration to the host in the non-preferred fault domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image15.png&#34; alt=&#34;Test Environment VM Placement Post Script&#34;&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned before, there are several use cases I can think of for pinning a VM to a specific site. The most obvious one is a VM that is not replicated across AZs, so it makes sense to have the VM running in the same AZ. The second one is a resilient application, such as AD Domain Controllers, where making sure at least one node is always running. The great thing about compute policies is that they leverage a ‘should’ rule. That means that if an entire AZ fails, for example the non-preferred one, if the VM data is replicated the VMs set to the MultiAZ-non-preferred compute policy will still be powered on the preferred site by vSphere HA. Once the non-preferred AZ comes back up DRS will migrate the VMs to hosts the adhere to the compute policy.&lt;/p&gt;
&lt;p&gt;I hope that you find this PowerShell module useful and that it can help address some design requirements in your SDDC. At the moment, in order to tag new hosts that are added to the SDDC using eDRS or as a planned procedure, you’ll need to run &lt;strong&gt;Set-MultiAzHostTag&lt;/strong&gt; again. Since it skips existing hosts you can also run it on a schedule. In the next couple of weeks, I’m planning on testing integration of the Set-MultiAzHostTag with &lt;ins&gt;
&lt;a href=&#34;https://vmweventbroker.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VEBA&lt;/a&gt;&lt;/ins&gt;, so it can be run when a new host is added to the SDDC.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
